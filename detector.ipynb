{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2362071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, Idefics2ForConditionalGeneration\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c823acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tokenizer will remove spaces so remove them for the labels and ensure they are real words.\n",
    "labels = [\"Fake\", \"Real\"]\n",
    "\n",
    "label_folders = {\n",
    "    \"Fake\": \"fake\",\n",
    "    \"Real\": \"real\"\n",
    "}\n",
    "\n",
    "dataset_folder = \"/Users/akash/Code/car_images/dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeeb9796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/akash/Code/car_images/dataset/fake/niss...</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=224x224 a...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/akash/Code/car_images/dataset/fake/niss...</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=224x224 a...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/akash/Code/car_images/dataset/fake/toyo...</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=224x224 a...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/akash/Code/car_images/dataset/fake/tesl...</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=224x224 a...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/akash/Code/car_images/dataset/fake/tesl...</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=224x224 a...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>/Users/akash/Code/car_images/dataset/real/chev...</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=224x224 a...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>/Users/akash/Code/car_images/dataset/real/bmw_...</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=224x224 a...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>/Users/akash/Code/car_images/dataset/real/bmw_...</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=224x224 a...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>/Users/akash/Code/car_images/dataset/real/chev...</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=224x224 a...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>/Users/akash/Code/car_images/dataset/real/ford...</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=224x224 a...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   id  \\\n",
       "0   /Users/akash/Code/car_images/dataset/fake/niss...   \n",
       "1   /Users/akash/Code/car_images/dataset/fake/niss...   \n",
       "2   /Users/akash/Code/car_images/dataset/fake/toyo...   \n",
       "3   /Users/akash/Code/car_images/dataset/fake/tesl...   \n",
       "4   /Users/akash/Code/car_images/dataset/fake/tesl...   \n",
       "..                                                ...   \n",
       "95  /Users/akash/Code/car_images/dataset/real/chev...   \n",
       "96  /Users/akash/Code/car_images/dataset/real/bmw_...   \n",
       "97  /Users/akash/Code/car_images/dataset/real/bmw_...   \n",
       "98  /Users/akash/Code/car_images/dataset/real/chev...   \n",
       "99  /Users/akash/Code/car_images/dataset/real/ford...   \n",
       "\n",
       "                                                image label  \n",
       "0   <PIL.Image.Image image mode=RGB size=224x224 a...  Fake  \n",
       "1   <PIL.Image.Image image mode=RGB size=224x224 a...  Fake  \n",
       "2   <PIL.Image.Image image mode=RGB size=224x224 a...  Fake  \n",
       "3   <PIL.Image.Image image mode=RGB size=224x224 a...  Fake  \n",
       "4   <PIL.Image.Image image mode=RGB size=224x224 a...  Fake  \n",
       "..                                                ...   ...  \n",
       "95  <PIL.Image.Image image mode=RGB size=224x224 a...  Real  \n",
       "96  <PIL.Image.Image image mode=RGB size=224x224 a...  Real  \n",
       "97  <PIL.Image.Image image mode=RGB size=224x224 a...  Real  \n",
       "98  <PIL.Image.Image image mode=RGB size=224x224 a...  Real  \n",
       "99  <PIL.Image.Image image mode=RGB size=224x224 a...  Real  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import PIL.Image\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "prompt = \"Classify the following image into exactly one of \" + \", \".join([f\"'{label}'\" for label in labels])\n",
    "\n",
    "def load_realwaste(path, sample_per_label = 200, training_percent = 0.9):\n",
    "    if not os.path.isdir(path):\n",
    "        raise Exception(f\"{path} path does not exist\")\n",
    "    \n",
    "    label_dict = {}\n",
    "\n",
    "    for label in labels:\n",
    "        label_dict[label] = []\n",
    "        for file in glob(os.path.join(path, label_folders[label], \"*.jpg\")):\n",
    "            label_dict[label].append(file)\n",
    "    \n",
    "    train_ds = []\n",
    "    eval_ds = []\n",
    "    train_count = math.floor(sample_per_label * training_percent)\n",
    "    test_count = math.floor((sample_per_label - train_count) / 2)\n",
    "\n",
    "    for label in labels:\n",
    "        files = label_dict[label]\n",
    "\n",
    "        training_data = [{\"id\": file, \"image\": PIL.Image.open(f\"{file}\").resize((224, 224)), \"label\": label} for file in files[:train_count]]\n",
    "        eval_data = [{\"id\": file, \"image\": PIL.Image.open(f\"{file}\").resize((224, 224)), \"label\": label} for file in files[train_count: train_count + test_count]]\n",
    "\n",
    "        train_ds.extend(training_data)\n",
    "        eval_ds.extend(eval_data)\n",
    "\n",
    "    return train_ds, eval_ds, pd.DataFrame(train_ds), pd.DataFrame(eval_ds)\n",
    "\n",
    "train_ds, eval_ds, train_df, eval_df = load_realwaste(dataset_folder)\n",
    "display(train_df)\n",
    "display(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c6a7fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d284f40d1845f3b20742cfbf39455d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chat templates should be in a 'chat_template.jinja' file but found key='chat_template' in the processor's config. Make sure to move your template to its own file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557ca506ce2e427b9a97dc251a9f6d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/460 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf2820372d24349907e30aa9cbbbf0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79e103aa56742b3adf0168bf8b310d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0070e074a85a4798943c6693d90d40db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a741dc25ffda4f81beccd394a417004b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/92.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3320cb55e842bea82b9b95703d8c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"HuggingFaceM4/idefics2-8b\", do_image_splitting=False, resume_download=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f283e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bf69550df843aa94681b51261a1464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "The installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. You may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      1\u001b[39m bnb_config = BitsAndBytesConfig(\n\u001b[32m      2\u001b[39m     load_in_4bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      3\u001b[39m     bnb_4bit_quant_type=\u001b[33m\"\u001b[39m\u001b[33mnf4\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     bnb_4bit_compute_dtype=torch.float16\n\u001b[32m      5\u001b[39m     )\n\u001b[32m      7\u001b[39m lora_config = LoraConfig(\n\u001b[32m      8\u001b[39m     r=\u001b[32m16\u001b[39m,\n\u001b[32m      9\u001b[39m     lora_alpha=\u001b[32m8\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     init_lora_weights=\u001b[33m\"\u001b[39m\u001b[33mgaussian\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m model = \u001b[43mIdefics2ForConditionalGeneration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHuggingFaceM4/idefics2-8b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m model = get_peft_model(model, lora_config)\n\u001b[32m     25\u001b[39m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/car_images/lib/python3.13/site-packages/transformers/modeling_utils.py:311\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    309\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    313\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/car_images/lib/python3.13/site-packages/transformers/modeling_utils.py:4648\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4645\u001b[39m     hf_quantizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4647\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4648\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4650\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4654\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4655\u001b[39m     torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)\n\u001b[32m   4656\u001b[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/car_images/lib/python3.13/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:88\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version.parse(importlib.metadata.version(\u001b[33m\"\u001b[39m\u001b[33mbitsandbytes\u001b[39m\u001b[33m\"\u001b[39m)) < version.parse(\u001b[33m\"\u001b[39m\u001b[33m0.43.1\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.cuda.is_available():\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     89\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     90\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     91\u001b[39m         )\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[31mImportError\u001b[39m: The installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. You may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1."
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules='.*(text_model|modality_projection|perceiver_resampler).*(down_proj|gate_proj|up_proj|k_proj|q_proj|v_proj|o_proj).*$',\n",
    "    use_dora=False,\n",
    "    init_lora_weights=\"gaussian\"\n",
    "    )\n",
    "\n",
    "model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "    \"HuggingFaceM4/idefics2-8b\",\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    low_cpu_mem_usage = True,\n",
    "    resume_download = None,\n",
    "    )\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d73c2745",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 114\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     77\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: accuracy,\n\u001b[32m     78\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m\"\u001b[39m: precision,\n\u001b[32m     79\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m\"\u001b[39m: recall,\n\u001b[32m     80\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbad_predictions\u001b[39m\u001b[33m\"\u001b[39m: bad_prediction_count\n\u001b[32m     81\u001b[39m     }\n\u001b[32m     84\u001b[39m training_args = TrainingArguments(\n\u001b[32m     85\u001b[39m     num_train_epochs = \u001b[32m2\u001b[39m,\n\u001b[32m     86\u001b[39m     per_device_train_batch_size = \u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    110\u001b[39m     label_names = [\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    111\u001b[39m )\n\u001b[32m    113\u001b[39m trainer = Trainer(\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     model = \u001b[43mmodel\u001b[49m,\n\u001b[32m    115\u001b[39m     args = training_args,\n\u001b[32m    116\u001b[39m     data_collator = \u001b[38;5;28;01mlambda\u001b[39;00m samples: data_collator(processor, samples),\n\u001b[32m    117\u001b[39m     train_dataset = train_ds,\n\u001b[32m    118\u001b[39m     eval_dataset = eval_ds,\n\u001b[32m    119\u001b[39m     compute_metrics = \u001b[38;5;28;01mlambda\u001b[39;00m predictions: compute_metrics(processor, predictions),\n\u001b[32m    120\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def data_collator(processor, samples):\n",
    "    texts = []\n",
    "    images = []\n",
    "\n",
    "    image_token_id = processor.tokenizer.additional_special_tokens_ids[processor.tokenizer.additional_special_tokens.index(\"<image>\")]\n",
    "\n",
    "    for sample in samples:\n",
    "        image = sample[\"image\"]\n",
    "        label = sample[\"label\"]\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"image\"},\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": label}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        text = processor.apply_chat_template(messages, add_generation_prompt=False)\n",
    "        texts.append(text.strip())\n",
    "        images.append([image])\n",
    "\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    batch_labels = batch[\"input_ids\"].clone()\n",
    "    batch_labels[batch_labels == processor.tokenizer.pad_token_id] = image_token_id\n",
    "    batch[\"labels\"] = batch_labels\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "def compute_metrics(processor, pred):\n",
    "    batch_labels = []\n",
    "    for label_id_batch in pred.label_ids:\n",
    "        for label_id in label_id_batch:\n",
    "            label = processor.batch_decode(label_id, skip_special_tokens=True)\n",
    "            label = \"\".join(label).split(\"Assistant:\")[-1].strip()\n",
    "            batch_labels.append(label)\n",
    "\n",
    "    bad_prediction_count = 0\n",
    "    bad_predictions = {}\n",
    "    predictions = []\n",
    "    i = 0\n",
    "    for prediction_batch in pred.predictions:\n",
    "        for prediction_tensor in prediction_batch[0]:\n",
    "            prediction_id = prediction_tensor.argmax(-1)\n",
    "            prediction_answer = processor.batch_decode(prediction_id, skip_special_tokens=True)\n",
    "            prediction = \"\".join(prediction_answer).split(\"Assistant:\")[-1].strip()\n",
    "            predictions.append(prediction)\n",
    "            \n",
    "            if not prediction in labels:\n",
    "                bad_prediction_count += 1\n",
    "                label = batch_labels[i]\n",
    "                if not label in bad_predictions:\n",
    "                    bad_predictions[label] = [prediction]\n",
    "                else:\n",
    "                    bad_predictions[label].append(prediction)\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "\n",
    "    accuracy = accuracy_score(batch_labels, predictions)\n",
    "    precision = precision_score(batch_labels, predictions, average='weighted', zero_division=0.0)\n",
    "    recall = recall_score(batch_labels, predictions, average='weighted', zero_division=0.0)\n",
    "\n",
    "    print(f\"Bad predictions:\\n{bad_predictions}\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"bad_predictions\": bad_prediction_count\n",
    "    }\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs = 2,\n",
    "    per_device_train_batch_size = 1,\n",
    "    per_device_eval_batch_size = 1,\n",
    "    gradient_accumulation_steps = 10,\n",
    "    gradient_checkpointing = True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant':False},\n",
    "    warmup_steps = 50,\n",
    "    learning_rate = 1e-4,\n",
    "    weight_decay = 0.01,\n",
    "    logging_steps = 10,\n",
    "    logging_strategy = \"steps\",\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps = 10,\n",
    "    save_total_limit = 100,\n",
    "    eval_do_concat_batches = False,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = \"accuracy\",\n",
    "    overwrite_output_dir = True,\n",
    "    fp16 = True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to = \"none\",\n",
    "    eval_strategy = \"steps\",\n",
    "    eval_steps = 10,\n",
    "    output_dir = \"checkpoints\",\n",
    "    log_level = \"info\",\n",
    "    label_names = [\"labels\"],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    data_collator = lambda samples: data_collator(processor, samples),\n",
    "    train_dataset = train_ds,\n",
    "    eval_dataset = eval_ds,\n",
    "    compute_metrics = lambda predictions: compute_metrics(processor, predictions),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b08eeb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m.config.use_cache=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m      2\u001b[39m trainer.train()\n\u001b[32m      3\u001b[39m model.config.use_cache = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.config.use_cache=False\n",
    "trainer.train()\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "259ecfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_inference_metrics(df_cm, y_true):\n",
    "    metrics = []\n",
    "    total_tp_tn = 0\n",
    "\n",
    "    for label in set(y_true):\n",
    "        df_label = df_cm.loc[[label]]\n",
    "        tp = df_label[label].iloc[0]\n",
    "        fp = df_label.sum(axis=1).iloc[0] - tp\n",
    "        fn = df_cm[label].sum() - tp\n",
    "        total_tp_tn += tp\n",
    "\n",
    "        precision = tp / (tp+fp)\n",
    "        recall = tp / (tp+fn)\n",
    "\n",
    "        metrics.append({\"label\": label, \"precision\": precision, \"recall\": recall})\n",
    "\n",
    "    accuracy = (total_tp_tn / len(y_true)) * 100\n",
    "    df_metrics = pd.DataFrame(metrics)\n",
    "    df_metrics.set_index(\"label\")\n",
    "\n",
    "    return df_metrics, accuracy\n",
    "\n",
    "\n",
    "def infer_model(processor, model, image):\n",
    "    messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"image\"},\n",
    "                ]\n",
    "            },       \n",
    "        ]\n",
    "\n",
    "    input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = processor(text=input_text, images=[image], return_tensors=\"pt\")\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "    generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return generated_texts[0].split(\"Assistant:\")[-1].strip()\n",
    "\n",
    "\n",
    "def evaluate_model(processor, model, eval_dataset):\n",
    "    model.eval()\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    images = []\n",
    "    \n",
    "    bad_prediction_count = 0\n",
    "    bad_predictions = {}\n",
    "\n",
    "    for sample in eval_dataset:\n",
    "        image = sample[\"image\"]\n",
    "        label = sample[\"label\"]\n",
    "\n",
    "        prediction = infer_model(processor, model, image)\n",
    "\n",
    "        y_true.append(label)\n",
    "        y_pred.append(prediction)\n",
    "        images.append(image)\n",
    "\n",
    "        if not prediction in labels:\n",
    "            bad_prediction_count += 1\n",
    "            if not label in bad_predictions:\n",
    "                bad_predictions[label] = [prediction]\n",
    "            else:\n",
    "                bad_predictions[label].append(prediction)\n",
    "\n",
    "    labels_set = sorted(set(y_true))\n",
    "    cm = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=labels)\n",
    "    df_cm = pd.DataFrame(cm, index=labels, columns=labels_set)\n",
    "\n",
    "    df_metrics, accuracy = compute_inference_metrics(df_cm, y_true)\n",
    "\n",
    "    return y_true, y_pred, images, df_cm, df_metrics, accuracy, bad_prediction_count, bad_predictions\n",
    "\n",
    "\n",
    "def display_sample_failures(df, label, prediction):\n",
    "    images = df[(df[\"Label\"] == label) & (df[\"Prediction\"] == prediction)][\"Image\"]\n",
    "    print(f\"Found {len(images)} samples with label '{label}' and prediction '{prediction}'\")\n",
    "\n",
    "    for i in range(min(5, len(images))):\n",
    "        display(images.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbbfc96d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m y_true, y_pred, images, df_cm, df_metrics, accuracy, bad_prediction_count, bad_predictions = evaluate_model(processor, \u001b[43mmodel\u001b[49m, eval_ds)\n\u001b[32m      2\u001b[39m df_results = pd.DataFrame(\u001b[38;5;28mzip\u001b[39m(y_true, y_pred, images), columns=[\u001b[33m\"\u001b[39m\u001b[33mLabel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPrediction\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mImage\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAccuracy:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "y_true, y_pred, images, df_cm, df_metrics, accuracy, bad_prediction_count, bad_predictions = evaluate_model(processor, model, eval_ds)\n",
    "df_results = pd.DataFrame(zip(y_true, y_pred, images), columns=[\"Label\", \"Prediction\", \"Image\"])\n",
    "\n",
    "print(f\"Accuracy:\\t\\t{accuracy:.2f}\")\n",
    "print(f\"Bad Predictions:\\t{bad_prediction_count}/{len(y_true)}\")\n",
    "print(bad_predictions)\n",
    "\n",
    "plt.figure()\n",
    "sn.heatmap(df_cm, annot=True, fmt=\"d\", cmap='BuGn')\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"Label\")\n",
    "plt.show()\n",
    "display(df_cm)\n",
    "\n",
    "display(df_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b428e094",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m display_sample_failures(\u001b[43mdf_results\u001b[49m, label=\u001b[33m\"\u001b[39m\u001b[33mMiscellaneous\u001b[39m\u001b[33m\"\u001b[39m, prediction=\u001b[33m\"\u001b[39m\u001b[33mPaper\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df_results' is not defined"
     ]
    }
   ],
   "source": [
    "display_sample_failures(df_results, label=\"Miscellaneous\", prediction=\"Paper\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "car_images",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
